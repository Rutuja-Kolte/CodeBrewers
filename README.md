# Gesture Detector

### Team name: CodeBrewers

## Team members
* Rutuja Ajay Kolte - kolterutuja1@gmail.com
* Mahek Ajay Salia - saliamahek13@gmail.com
* Reshmika Sreenath Nambiar - reshmikasnambiar@gmail.com
* Prerna Jagesia - pkjagesia@gmail.com

## Mentors
* Anuj Raghani
* Bhavya Sheth
* Owais Hetavkar
* Vedant Paranjape

## Description
The goal of our project was to train a machine learning algorithm capable of classifying images of different hand gestures (such as fists, palm, etc.) and use it for gesture detection and recognition.
* GitHub repo link: [Link to repository](https://github.com/Rutuja-Kolte/CodeBrewers)
* Drive link: [Drive link here](https://drive.google.com/drive/folders/1YxJxfa36NaUZYAGVKP1biX78SaF2loQw?usp=sharing)

## Technology stack

Tools and technologies that we learnt and used in the project.

1. Python
2. Open CV and CNN
3. Jupyter notebook
4. Machine learning

## Project Setup
### Method 1  
1. Clone the CodeBrewers repository  
```bash
    git clone https://github.com/Rutuja-Kolte/CodeBrewers
```
2. Open Google Drive and create a folder named CodeBrewers.
3. Upload all files from the CodeBrewers repository on your PC to Google Drive.
4. Also add the [dataset from Kaggle](https://www.kaggle.com/gti-upm/leapgestrecog/version/1) and name it proj.zip

  
### Method 2  
1. Clone the CodeBrewers repository  
```bash
    git clone https://github.com/Rutuja-Kolte/CodeBrewers
```
2. Go to the [drive link](https://drive.google.com/drive/folders/1YxJxfa36NaUZYAGVKP1biX78SaF2loQw?usp=sharing) and copy the folder and save it in your own drive.
## Usage
### To Create the Model (skip this if you want to use the pre-trained model)
#### Method 1
1. Right click on CodeBrewers.ipynb file in Google Drive.
2. Click on open with Google Colab.
3. Run the code.  
#### Method 2
1. Open CodeBrewers.ipynb from the CodeBrewers repository in Google Colab.
2. Run the code.

### To Use the Model
#### Method 1
1. Right click on GestureDetector.ipynb file in Google Drive.
2. Click on open with Google Colab.
3. Run the code.  
#### Method 2
1. Open GestureDetector.ipynb from the CodeBrewers repository in Google Colab.
2. Run the code.

## Applications
1. Touchless user interface is an emerging type of technology in relation to gesture control. One type of touchless interface uses the bluetooth connectivity of a smartphone to activate a company's visitor management system. This prevents having to touch an interface during the COVID-19 pandemic.
2. Hand gesture recognition has great value in sign language recognition and sign language interpreters for the disabled.
3. In cranes, this can be used instead of remotes so that easy picking and shedding of load can be done at difficult locations.

## Future scope
The project can be linked to a Media player such as VLC and the gestures can be used to control the video like increasing or decreasing its volume or fast forwarding and rewinding the video. Also, instead of using a mouse the gestures can also be used to control your mouse pointer.  
  
Currently, the model used cannot recognise when there are no gestures detected. This functionality can be added as well.   
  
In the above project we have used only static gestures. It can be modified to include dynamic gestures (swiping your fist to the right or left, moving your finger up and down, etc.).

## Screenshots
Add a few screenshots here to give the viewer a quick idea of what your project looks like. After all, a picture speaks a thousand words.

You'll have to link the screenshots from your drive folder here.

![Screenshot alt text](https://edtimes.in/wp-content/uploads/2018/09/NikeMeme10-640x633.jpg "Here is a screenshot")

Use this template as a guide for writing your documentation. Feel free to customize and adapt it for you project.
For more Markdown syntax help, visit [here](https://www.markdownguide.org/basic-syntax/)
